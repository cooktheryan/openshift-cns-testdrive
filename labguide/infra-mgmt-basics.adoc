[abstract]
Overview
--------
In this lab you are introduced to some basic management concepts of the OpenShift Container Platform. This is largely driven by automation through openshift-ansible which makes operations easy and predictable. +
By the end of this module you should have your cluster extended by another 3 nodes, 1 nodes briefly put into maintenance and the cluster pruned of unnecessary data.

## Overview of Logging and Metrics

### Deploying and using Metrics

_Metrics_ in OpenShift refers to the continuous collection of performance and utilization data of pods and nodes in the cluster. It allows for centralized monitoring in the OpenShift UI and automated horizontal scaling of pods based on utilization.

The metrics implementation is based on http://www.hawkular.org/[Hawkular], a metrics collection system running on OpenShift persisting data in a Cassandra database.

In your environment metrics is not yet deployed. Configuration is done by customizing the Ansible inventory file `/etc/ansible/hosts` and deployment is facilitated by running a specific playbook that is part of the `openshift-ansible` installer.

Modify (update or add) the inventory settings in the `[OSEv3:vars]` section as follows to configure Metrics deployment:

[source,ini]
./etc/ansible/hosts
----

...

[OSEv3:vars]
...
openshift_metrics_install_metrics=true
openshift_metrics_cassandra_storage_type=pv
openshift_metrics_cassandra_pvc_size=10Gi
openshift_metrics_cassanda_pvc_storage_class_name={{ CNS_STORAGECLASS }}
openshift_metrics_hawkular_hostname=metrics.{{ OCP_ROUTING_SUFFIX }}

...
----

Then run the `openshift-metrics` playbook on the master node:

----
[cloud-user@{{MASTER_HOSTNAME}} ~]$ ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml
----

This will deploy the metric collection and visualization stack on OpenShift. All resources will be stood up in the `openshift-infra` project. As part of the deployment persistent storage will be provisioned to host to store the metrics.

NOTE: In this environment the persistent storage for the metrics store Cassandra will come from CNS. While functionally working it is at this time not recommended to store the metrics data on CNS for production environment. This is currently under testing and in parallel a more efficient CNS implementation for metrics and logging data in OpenShift is being developed.

CAUTION: In this lab environment it can take up to 2-3 minutes after the metrics playbook finishes for the Hawkular stack to finish intialization. During this time you might see this error message in the UI: image::openshift-metrics-url-error.png[] +
This will disappear as soon as the hawkular-metrics pods are in _Ready_ state.

As cluster admin, verify the metrics components are running in the `openshift-infra` namespace:

----
[cloud-user@{{MASTER_HOSTNAME}} ~]$ oc login -u system:admin -n openshift-infra
Logged into "https://{{ MASTER_INTERNAL_FQDN }}/:443" as "system:admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    {{ CNS_NAMESPACE }}
    default
    kube-system
    logging
    management-infra
    openshift
  * openshift-infra

Using project "openshift-infra".
[cloud-user@{{MASTER_HOSTNAME}} ~]$ oc get pods
NAME                         READY     STATUS    RESTARTS   AGE
hawkular-cassandra-1-c30h6   1/1       Running   0          10m
hawkular-metrics-0c5c3       1/1       Running   0          10m
heapster-sv95t               1/1       Running   0          10m
----

In the OpenShift UI ({{ WEB_CONSOLE_URL }}) you can see metric graphs appearing next to `DeploymentConfig` object:

.The OpenShift UI will show history metrics for applications
image::openshift-metrics-overview.png[]

IMPORTANT: If instead of metrics charts an error message is displayed, stating that the metrics URL could not be reached this is due to your browser not trusting the  self-signed certificates used in this environment. +
Click the metrics URL https://metrics.{{ OCP_ROUTING_SUFFIX }}/ once and accept the untrusted certificate. Once you return to the OpenShift UI the graphs should start to appear.

NOTE: It might be necessary for you to refresh your browser to see the charts.

In the context of a specific pod the *Metrics* tab in the UI will show metrics for this particular pod only with a configurable time-range. Also optionally a _donut_ chart next to a resource appears if the pods was given a consumption limit on this resource (e.g. RAM).

image::openshift-metrics-pods.png[]

### Deploying and using Logging

Equally important to performance metrics is collecting and aggregating logs from the environments and the application pods it is running. OpenShift ships with an elastic log aggregation solution: *EFK*. +
**E**lasticSearch, **F**luentd and **K**ibana forms a configuration where logs from all nodes and applications are consolidated (Fluentd) in a central place (ElasticSearch) on top of which rich queries can be made from a single UI (Kibana). Administrators can see and search through all logs, application owners and developers can allow access logs that belong to their projects. +
Like metrics the EFK stack runs on top of OpenShift.

To configure EFK edit (update or insert) the Ansible inventory file `/etc/ansible/hosts` to contain the following settings in the `[OSEv3:vars]` section:


[source,ini]
./etc/ansible/hosts
----

...

[OSEv3:vars]
...
openshift_hosted_logging_deploy=true
openshift_logging_namespace=logging
openshift_logging_elasticsearch_pvc_storage_class_name={{CNS_STORAGECLASS}}
openshift_logging_elasticsearch_storage_type=pvc
openshift_logging_elasticsearch_pvc_size=10Gi
...
----

With these settings in place executing the `openshift-logging` Ansible playbook that ships as part of the `openshift-ansible` installer:

  [cloud-user@{{MASTER_HOSTNAME}} ~]$ ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml

The logging components will be deployed in the `logging` namespace. If you are not already, log in as cluster admin and switch into that namespace:

  [cloud-user@{{MASTER_HOSTNAME}} ~]$ oc login -u system:admin -n logging

Verify the logging stack components are up and running:

----
[cloud-user@{{MASTER_HOSTNAME}} ~]$ oc get pods
NAME                          READY     STATUS    RESTARTS   AGE
logging-curator-1-bm14r       1/1       Running   0          7m
logging-es-wihwmf7x-1-kpsc0   1/1       Running   0          7m
logging-fluentd-frtkj         1/1       Running   0          7m
logging-fluentd-hzzmh         1/1       Running   0          7m
logging-fluentd-mhjgb         1/1       Running   0          7m
logging-fluentd-nxb40         1/1       Running   0          7m
logging-fluentd-xl7l0         1/1       Running   0          7m
logging-kibana-1-76vh4        2/2       Running   0          7m
----

The _Fluentd_ pods are deployed as part of a `DaemonSet` to have a log shipping component deployed on every node in the cluster:

----
[cloud-user@{{MASTER_HOSTNAME}}  ~]$ oc get daemonset
NAME              DESIRED   CURRENT   READY     NODE-SELECTOR                AGE
logging-fluentd   5         5         5         logging-infra-fluentd=true   9m
----

To reach the _Kibana_ user interface, first determine it's public access URL by querying the route that got set up to expose this service:

----
[cloud-user@{{MASTER_HOSTNAME}} ~]$ oc get route/logging-kibana
NAME             HOST/PORT                                              PATH      SERVICES         PORT      TERMINATION          WILDCARD
logging-kibana   kibana.apps.{{ OCP_ROUTING_SUFFIX }}             logging-kibana   <all>     reencrypt/Redirect   None
----

As stated above the UI should be reachable via https://kibana.apps.{{ OCP_ROUTING_SUFFIX }}/ - login with OpenShift user credentials of either an administrative account or a user account.

The _Kibana_ user interface appears, providing reach controls to search through logs from all over the cluster.

image::openshift-logging-kibana-ui.png[]
